{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5568a0d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jonathan Ellisor\\OneDrive - aft.org\\AFTDBFileUpload\\Fuzzymatcher\\Workbench\\01018_23255\\01018_23255_20211028_Knackbuild.xlsx\n",
      "C:\\Users\\Jonathan Ellisor\\OneDrive - aft.org\\AFTDBFileUpload\\Fuzzymatcher\\Workbench\\01018_23255\\01018_EntityList.xlsx\n",
      "Neither membership nor entity file data for ('JobTitleName', 'JobTitle') exists.\n",
      "No spreadsheet created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jonathan Ellisor\\anaconda3\\lib\\site-packages\\recordlinkage\\base.py:139: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  rr = 1 - n / n_max\n",
      "C:\\Users\\Jonathan Ellisor\\anaconda3\\lib\\site-packages\\recordlinkage\\base.py:155: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  rr_avg = 1 - n_total / n_max_total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both membership and entity file data exist for LocalJobClassName. Fuzzy match spreadsheet created.\n",
      "Both membership and entity file data exist for WorkLocationName. Fuzzy match spreadsheet created.\n",
      "Neither membership nor entity file data for ('WorkStructureName', 'WorkStructure') exists.\n",
      "No spreadsheet created.\n",
      "Both membership and entity file data exist for LocalDuesCategoryName. Fuzzy match spreadsheet created.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import recordlinkage\n",
    "import re\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import os.path\n",
    "import configparser\n",
    "\n",
    "\n",
    "#takes in entity file path and creates dataframe\n",
    "def entity_file_creator(entity_file_path, ws_name):\n",
    "    entity_sheet = pd.read_excel(entity_file_path, ws_name)\n",
    "    col_name_list = re.findall('[A-Z][^A-Z]*', ws_name)\n",
    "    col_name = ' '.join(col_name_list) + ' Name'\n",
    "    if (len(entity_sheet[col_name].dropna()) > 0) == True:\n",
    "        col_id = ' '.join(col_name_list) + ' Id'\n",
    "        data = {col_name : entity_sheet[col_name].str.rstrip(), col_id: entity_sheet[col_id]}\n",
    "        if ws_name == 'JobTitle' or ws_name == 'LocalJobClass': \n",
    "            if ((len(entity_sheet['Unit Id'].dropna()) > 0) == True): \n",
    "                data['Unit Id'] = entity_sheet['Unit Id'].values   \n",
    "        elif ws_name == 'WorkLocation' or ws_name == 'WorkStructure': \n",
    "            if ((len(entity_sheet['Employer Id'].dropna()) > 0) == True): \n",
    "                data['Employer Id'] = entity_sheet['Employer Id'].values  \n",
    "        return pd.DataFrame(data).dropna()    \n",
    "    \n",
    "#takes in knackbuild file path and creates dataframe\n",
    "def member_id_creator(member_file_path, col_name):\n",
    "    member_file = pd.read_excel(member_file_path)\n",
    "    if (len(member_file[col_name].dropna()) > 0) == True:\n",
    "        block_type = ''\n",
    "        if col_name == 'JobTitleName' or col_name == 'LocalJobClassName': \n",
    "            block_type = 'UnitId'\n",
    "        elif col_name == 'WorkLocationName' or col_name == 'WorkStructureName':\n",
    "            block_type = 'EmployerId'\n",
    "        elif col_name == 'LocalDuesCategoryName':\n",
    "            block_type = 'UnionRelationshipTypeName' #we don't actually need this data... just didn't wanna rewrite the program\n",
    "        col_and_units_name = str(col_name) + \" + \" + block_type\n",
    "        member_file[col_and_units_name] = member_file[col_name].astype(str) + member_file[block_type].astype(str)\n",
    "        unique_entries_index = member_file[col_and_units_name].drop_duplicates().index\n",
    "        member_descriptions = member_file.loc[unique_entries_index, col_name].values\n",
    "        units = member_file.loc[unique_entries_index, block_type].values\n",
    "        descriptions_and_units = member_file.loc[unique_entries_index, col_and_units_name].values\n",
    "        data = {col_name:member_descriptions, block_type:units, col_and_units_name:descriptions_and_units}\n",
    "        return pd.DataFrame(data) \n",
    "    \n",
    "#cleans up column in knackbuild file so match accuracy is improved\n",
    "#each local has unique cleaning instructions\n",
    "def clean(col_name, member_df, member_file_path):\n",
    "    local_num = str(member_file_path)[-36:-31]\n",
    "    cleaning_dict_path = str(member_file_path)[:-59] + \"\\cleaning_dictionaries\\{}_cleaning_dict.csv\".format(local_num)\n",
    "        \n",
    "    cleaning_dict = {}  \n",
    "    with open(cleaning_dict_path) as file:\n",
    "        cleaning_dict_csv = csv.reader(file, delimiter=',')\n",
    "        for line in cleaning_dict_csv:\n",
    "            cleaning_dict[line[0]] = line[1]\n",
    "            #print(cleaning_dict) #FOR TESTING\n",
    "\n",
    "    member_df[col_name+'_clean'] = member_df[col_name]\n",
    "    [member_df[col_name+'_clean'].replace(key, value, regex=True,inplace=True) for key, value in cleaning_dict.items()]\n",
    "    member_df[col_name+'_clean'] = member_df[col_name+'_clean'].str.rstrip(' ')\n",
    "    return member_df\n",
    "\n",
    "#performs the match\n",
    "def matcher(member_df, entity_df, mem_col_name, cleaned_mem_col_name, enti_col_name, enti_col_id): \n",
    "    #assigning the columns that match should be blocked on\n",
    "    #i.e. only match if the unitid column is the same\n",
    "    block_type = ''\n",
    "    block_type_spaces = ''  \n",
    "    if mem_col_name == 'JobTitleName' or mem_col_name == 'LocalJobClassName': \n",
    "        block_type = 'UnitId'\n",
    "        block_type_spaces = 'Unit Id'\n",
    "    elif mem_col_name == 'WorkLocationName' or mem_col_name == 'WorkStructureName':\n",
    "        block_type = 'EmployerId'\n",
    "        block_type_spaces = 'Employer Id'\n",
    "    \n",
    "    if (block_type in member_df.columns) and (block_type_spaces in entity_df.columns):\n",
    "        #perfect matching\n",
    "        perf_matches = member_df.merge(entity_df, how='inner', left_on=[cleaned_mem_col_name, block_type], right_on=[enti_col_name, block_type_spaces])\n",
    "        member_wo_perf_matches = member_df[~member_df[mem_col_name + \" + \" + block_type].isin(perf_matches[mem_col_name + \" + \" + block_type])]\n",
    "        entity_wo_perf_matches = entity_df[~entity_df[enti_col_id].isin(perf_matches[enti_col_id])]\n",
    "        \n",
    "        #fuzzy matching\n",
    "        member_wo_perf_matches.set_index(mem_col_name + \" + \" + block_type,inplace=True)\n",
    "        entity_wo_perf_matches.set_index(enti_col_id,inplace=True)\n",
    "        indexer = recordlinkage.Index()\n",
    "        indexer.block(left_on=block_type, right_on=block_type_spaces)\n",
    "        candidates = indexer.index(member_wo_perf_matches, entity_wo_perf_matches)\n",
    "        compare = recordlinkage.Compare()\n",
    "        compare.string(cleaned_mem_col_name,enti_col_name,threshold=0.6,label='similarity')\n",
    "        features = compare.compute(candidates, member_wo_perf_matches, entity_wo_perf_matches)\n",
    "        potential_matches = features[features.sum(axis=1) == 1].reset_index()\n",
    "\n",
    "        entity_lu = entity_wo_perf_matches[[enti_col_name, block_type_spaces]].reset_index()\n",
    "        member_lu = member_wo_perf_matches[[cleaned_mem_col_name, mem_col_name, block_type]].reset_index()\n",
    "        entity_merge = potential_matches.merge(entity_lu, how='outer')\n",
    "        fuzzy_matches = entity_merge.merge(member_lu, how='right').drop(['similarity'],axis=1) \n",
    "    \n",
    "        if len(perf_matches) !=  0: perf_matches['type'] = ['Perfect Match']*len(perf_matches) \n",
    "        if len(fuzzy_matches) !=  0:\n",
    "            fuzzy_matches.loc[fuzzy_matches[mem_col_name].duplicated(keep=False) == False, 'type'] = 'One-to-One Fuzzy Match'\n",
    "            fuzzy_matches.loc[fuzzy_matches[mem_col_name].duplicated(keep=False) == True, 'type'] = 'Multiple Fuzzy Matches'\n",
    "            fuzzy_matches.loc[fuzzy_matches[enti_col_id].isna(), 'type'] = 'No Match to Entity File Found'\n",
    "    \n",
    "        matched_df = pd.concat([fuzzy_matches,perf_matches]).sort_values(by=['type'])\n",
    "        matched_df.rename(columns={mem_col_name:mem_col_name + ' Knackbuild',\n",
    "                               enti_col_name:enti_col_name+ ' Entity File',\n",
    "                               block_type: block_type + ' Knackbuild', \n",
    "                               block_type_spaces:block_type_spaces + ' Entity File'},inplace=True)\n",
    "        return matched_df\n",
    "\n",
    "    else:\n",
    "        print('ERROR: {} data does not exist in both the entity and membership file but it should.'.format(block_type))\n",
    "        print('Clean file and try again.')\n",
    "\n",
    "#merges knackbuild match column entries to entity file match column entries \n",
    "#for localduescategory\n",
    "#needs separate function because this is a single column match\n",
    "def matcher_localduescategory(member_df, entity_df, mem_col_name, cleaned_mem_col_name, enti_col_name, enti_col_id):\n",
    "    #perfect matching\n",
    "    perf_matches = member_df.merge(entity_df, how='inner', left_on=[cleaned_mem_col_name], right_on=[enti_col_name])\n",
    "    if len(perf_matches) !=  0: \n",
    "        perf_matches['type'] = ['Perfect Match']*len(perf_matches) \n",
    "\n",
    "    member_wo_perf_matches = member_df[~member_df[mem_col_name].isin(perf_matches[mem_col_name])]\n",
    "    entity_wo_perf_matches = entity_df[~entity_df[enti_col_id].isin(perf_matches[enti_col_id])]\n",
    "    member_wo_perf_matches = member_wo_perf_matches.dropna(subset=[mem_col_name],axis=0)\n",
    "    \n",
    "    #fuzzy matching\n",
    "    if len(member_wo_perf_matches[mem_col_name].dropna()) > 0 == True:\n",
    "        member_wo_perf_matches.set_index(mem_col_name,inplace=True)\n",
    "        entity_wo_perf_matches.set_index(enti_col_id,inplace=True)\n",
    "        indexer = recordlinkage.Index()\n",
    "        indexer.full()\n",
    "    \n",
    "        candidates = indexer.index(member_wo_perf_matches, entity_wo_perf_matches)\n",
    "        compare = recordlinkage.Compare()\n",
    "        compare.string(cleaned_mem_col_name,enti_col_name,threshold=0.6,label='similarity')\n",
    "        features = compare.compute(candidates, member_wo_perf_matches, entity_wo_perf_matches)\n",
    "        potential_matches = features[features.sum(axis=1) == 1].reset_index()\n",
    "\n",
    "        entity_lu = entity_wo_perf_matches[[enti_col_name]].reset_index()\n",
    "        member_lu = member_wo_perf_matches[[cleaned_mem_col_name, mem_col_name]].reset_index()\n",
    "        entity_merge = potential_matches.merge(entity_lu, how='outer')\n",
    "        fuzzy_matches = entity_merge.merge(member_lu, how='right').drop(['similarity'],axis=1)\n",
    "        \n",
    "        if len(fuzzy_matches) !=  0:\n",
    "            fuzzy_matches.loc[fuzzy_matches[mem_col_name].duplicated(keep=False) == False, 'type'] = 'One-to-One Fuzzy Match'\n",
    "            fuzzy_matches.loc[fuzzy_matches[mem_col_name].duplicated(keep=False) == True, 'type'] = 'Multiple Fuzzy Matches'\n",
    "            fuzzy_matches.loc[fuzzy_matches[enti_col_id].isna(), 'type'] = 'No Match to Entity File Found'\n",
    "    \n",
    "        matched_df = pd.concat([fuzzy_matches,perf_matches]).sort_values(by=['type'])\n",
    "        matched_df.rename(columns={mem_col_name:mem_col_name + ' Knackbuild',\n",
    "                           enti_col_name:enti_col_name+ ' Entity File'},inplace=True)\n",
    "        return matched_df\n",
    "    \n",
    "    perf_matches.rename(columns={mem_col_name:mem_col_name + ' Knackbuild',\n",
    "        enti_col_name:enti_col_name+ ' Entity File'},inplace=True)\n",
    "    return perf_matches\n",
    "\n",
    "#pulls together all helper functions to produce _match spreadsheets for review\n",
    "def runner(member_file_path, entity_file_path, member_entity_category_tuple):        \n",
    "    mem_name = member_entity_category_tuple[0]\n",
    "    enti_name = re.sub(r\"(\\w)([A-Z])\", r\"\\1 \\2\", member_entity_category_tuple[1]) + ' Name'\n",
    "    enti_id = re.sub(r'Name$',\"\",enti_name) + 'Id'\n",
    "\n",
    "    member = member_id_creator(member_file_path, mem_name)\n",
    "    entity = entity_file_creator(entity_file_path, member_entity_category_tuple[1])\n",
    "            \n",
    "    if entity is not None and member is not None:\n",
    "        member = clean(member_df=member, col_name = mem_name,member_file_path=member_file_path)\n",
    "        cleaned_mem_name = mem_name+'_clean'\n",
    "    \n",
    "        if mem_name == 'JobTitleName' or mem_name == 'LocalJobClassName' or mem_name == 'WorkLocationName' or mem_name == 'WorkStructureName': \n",
    "            matched_df = matcher(member_df=member, entity_df=entity, mem_col_name=mem_name, \n",
    "                                cleaned_mem_col_name=cleaned_mem_name, enti_col_name=enti_name, \n",
    "                                enti_col_id=enti_id)\n",
    "        elif mem_name == 'LocalDuesCategoryName':\n",
    "            matched_df = matcher_localduescategory(member_df=member, entity_df=entity, mem_col_name=mem_name, \n",
    "                                                  cleaned_mem_col_name=cleaned_mem_name, enti_col_name=enti_name, \n",
    "                                                  enti_col_id=enti_id)\n",
    "        \n",
    "        matched_df.dropna(subset=[mem_name + ' Knackbuild'],inplace=True)\n",
    "        matched_df.to_csv(str(member_file_path)[:-24] + \"{}_match.csv\".format(mem_name),index=False)\n",
    "        print('Both membership and entity file data exist for {}. Fuzzy match spreadsheet created.'.format(mem_name))    \n",
    "\n",
    "    elif entity is None and member is not None:\n",
    "        member.to_csv(str(member_file_path)[:-24] + \"{}_member.csv\".format(member_entity_category_tuple[0]),index=False)\n",
    "        print('The membership file data exists for {} but entity file data does not.'.format(member_entity_category_tuple))\n",
    "        print('A spreadsheet of entries in the membership file has been created.')\n",
    "        print('The affiliate may need to be contacted to create entity file categories.')\n",
    "    elif entity is not None and member is None:\n",
    "        print('The entity file data exists for {} but membership file data does not.'.format(member_entity_category_tuple))\n",
    "        print('No spreadsheet created.')\n",
    "    else:\n",
    "        print('Neither membership nor entity file data for {} exists.'.format(member_entity_category_tuple))\n",
    "        print('No spreadsheet created.')\n",
    "\n",
    "        \n",
    "def main():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.optionxform = str\n",
    "    config.read(os.path.expanduser(r\"~\\OneDrive - aft.org\\AFTDBFileUpload\\Fuzzymatcher\\config.ini\"))\n",
    "    categories = config['DEFAULT']\n",
    "    categories = [(k, v) for k, v in dict(categories).items()]\n",
    "    \n",
    "    directory = os.path.expanduser(r\"~\\OneDrive - aft.org\\AFTDBFileUpload\\Fuzzymatcher\\Workbench\") \n",
    "    pathlist = Path(directory).glob('**/*KnackBuild.xlsx')\n",
    "    for path in pathlist:\n",
    "        member_file_path = path\n",
    "        print(path)\n",
    "        entity_file_path = str(path)[:-30] + 'EntityList.xlsx'\n",
    "        print(entity_file_path)\n",
    "        entity_file_path = Path(entity_file_path)\n",
    "    \n",
    "        for category in categories:\n",
    "            runner(member_file_path, entity_file_path, category)\n",
    "            #except:\n",
    "            #   print('Error')\n",
    "            #  pass\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae3f6660",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jonathan Ellisor\\OneDrive - aft.org\\AFTDBFileUpload\\Fuzzymatcher\\Workbench\\01018_23255\\01018_23255_LocalDuesCategoryName_match.csv\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d5f897ad3b38>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mmerged_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmember_file_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-d5f897ad3b38>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_file\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m'Date'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                     \u001b[0mmerged_file\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerged_file\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mmerged_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmember_file_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5459\u001b[0m             \u001b[1;32mor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5460\u001b[0m         ):\n\u001b[1;32m-> 5461\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5462\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5463\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\accessor.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[1;31m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[0maccessor_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m         \u001b[1;31m# Replace the property with the accessor object. Inspired by:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[1;31m# https://www.pydanny.com/cached-property.html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\accessors.py\u001b[0m in \u001b[0;36m__new__\u001b[1;34m(cls, data)\u001b[0m\n\u001b[0;32m    492\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mPeriodProperties\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can only use .dt accessor with datetimelike values\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import re\n",
    "import os.path\n",
    "import numpy as np\n",
    "\n",
    "#creating new copy of membership file to work in so original file is preserved\n",
    "def copyer(directory):\n",
    "    pathlist_knackbuild = Path(directory).glob('**/*Knackbuild.xlsx')\n",
    "    for path in pathlist_knackbuild:\n",
    "        shutil.copy(path, str(path)[:-24] + 'Knackbuild_w_entityids.xlsx')\n",
    "\n",
    "#create strings that represent column names\n",
    "def name_creator(match_file_path):\n",
    "    match_col_name = str(match_file_path).split(\"_\")[3]\n",
    "    match_col_spaces = re.sub(r\"(\\w)([A-Z])\", r\"\\1 \\2\", match_col_name) #creating var for name with spaces\n",
    "    match_col_id_spaces = re.sub(r'Name$',\"\",match_col_spaces) + 'Id'\n",
    "    match_col_id_no_spaces = re.sub(r'Name$',\"\",match_col_name) + 'Id'\n",
    "    return {'match_col_name':match_col_name,'match_col_spaces':match_col_spaces,'match_col_id_no_spaces':match_col_id_no_spaces,'match_col_id_spaces':match_col_id_spaces}\n",
    "\n",
    "#remerging reviewed fuzzy match files back into knackbuild with ids from entity file\n",
    "def remerger(names_list,member_df,fuzzy_match_df):\n",
    "    block_type = ''\n",
    "    block_type_spaces = ''\n",
    "    if names_list['match_col_name'] == 'JobTitleName' or names_list['match_col_name'] == 'LocalJobClassName': \n",
    "        block_type = 'UnitId'\n",
    "        block_type_spaces = 'Unit Id'\n",
    "    elif names_list['match_col_name'] == 'WorkLocationName' or names_list['match_col_name'] == 'WorkStructureName':\n",
    "        block_type = 'EmployerId'\n",
    "        block_type_spaces = 'Employer Id'\n",
    "    fuzzy_match_df.rename(columns={names_list['match_col_spaces'] + ' Entity File': names_list['match_col_spaces'],\n",
    "                                   block_type_spaces + ' Entity File': block_type_spaces,\n",
    "                                   names_list['match_col_name'] + ' Knackbuild': names_list['match_col_name'], #possible to not do renaming later on by doing it here? hcange name to unitid so it automatically merges?\n",
    "                                   block_type + ' Knackbuild': block_type},inplace=True)\n",
    "    fuzzy_match_df.drop(['type',names_list['match_col_name'] + \"_clean\",block_type_spaces,names_list['match_col_name'] + \" + \" + block_type],axis=1,inplace=True)\n",
    "   \n",
    "    merged_file = member_df.merge(fuzzy_match_df, on = [names_list['match_col_name'],block_type], how='left')\n",
    "\n",
    "    merged_file[names_list['match_col_spaces']] = np.where(merged_file[names_list['match_col_spaces']] == np.nan, merged_file[names_list['match_col_name']], names_list['match_col_spaces'])\n",
    "    merged_file[names_list['match_col_id_no_spaces']] = merged_file[names_list['match_col_id_spaces']]\n",
    "    \n",
    "    return merged_file\n",
    "    \n",
    "def remerger_localduescategory(names_list,member_df,fuzzy_match_df):\n",
    "    fuzzy_match_df.rename(columns={names_list['match_col_spaces'] + ' Entity File': names_list['match_col_spaces'],\n",
    "                                   names_list['match_col_name'] + ' Knackbuild': names_list['match_col_name']}, inplace=True)\n",
    "    fuzzy_match_df.drop(['type',names_list['match_col_name'] + \"_clean\",names_list['match_col_name'] + \" + UnionRelationshipTypeName\"],axis=1,inplace=True)\n",
    "    \n",
    "    merged_file = member_df.merge(fuzzy_match_df, on = [names_list['match_col_name'],'UnionRelationshipTypeName'], how='left')\n",
    "    \n",
    "    merged_file[names_list['match_col_spaces']] = np.where(merged_file[names_list['match_col_spaces']] == np.nan, merged_file[names_list['match_col_name']], names_list['match_col_spaces'])\n",
    "    merged_file[names_list['match_col_id_no_spaces']] = merged_file[names_list['match_col_id_spaces']]\n",
    "    \n",
    "    return merged_file\n",
    "\n",
    "def main():\n",
    "    directory = os.path.expanduser(r\"~\\OneDrive - aft.org\\AFTDBFileUpload\\Fuzzymatcher\\Workbench\")\n",
    "\n",
    "    copyer(directory)\n",
    "    pathlist_fuzzymatch = Path(directory).glob('**/*_match.csv')\n",
    "    for path in pathlist_fuzzymatch:\n",
    "        print(path)\n",
    "        names_list = name_creator(match_file_path=path)\n",
    "\n",
    "        fuzzy_match_file = pd.read_csv(path)\n",
    "        member_file_path = str(path).split(\"_\")[:-2]\n",
    "        member_file_path = Path('_'.join(member_file_path) + '_Knackbuild_w_entityids.xlsx')\n",
    "        member_file = pd.read_excel(member_file_path)\n",
    "        \n",
    "        if names_list['match_col_name'] == 'JobTitleName' or names_list['match_col_name'] == 'LocalJobClassName' or names_list['match_col_name'] == 'WorkLocationName' or names_list['match_col_name'] == 'WorkStructureName': \n",
    "            merged_file = remerger(names_list=names_list,member_df=member_file,fuzzy_match_df=fuzzy_match_file)\n",
    "        elif names_list['match_col_name'] == 'LocalDuesCategoryName':\n",
    "            merged_file = remerger_localduescategory(names_list=names_list,member_df=member_file,fuzzy_match_df=fuzzy_match_file)\n",
    "    \n",
    "        merged_file.drop([names_list['match_col_spaces'],names_list['match_col_id_spaces']],axis=1,inplace=True)\n",
    "\n",
    "        for col in merged_file.columns:\n",
    "            if len(merged_file[col].dropna()) > 0:\n",
    "                if 'Date' in col:\n",
    "                    merged_file[col] = merged_file[col].dt.date\n",
    "        \n",
    "        merged_file.to_excel(member_file_path,index=False)\n",
    "        \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe316096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7ad955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
