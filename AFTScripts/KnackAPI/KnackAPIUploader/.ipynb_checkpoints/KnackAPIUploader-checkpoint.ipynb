{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7abbd7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "class KnackAFT:\n",
    "    def __init__(self):\n",
    "        # API #\n",
    "        self.API_VERSION = 'v1'\n",
    "        self.API_KEY = '1a210580-315e-11ea-a6a4-bb031a9e1ba1'\n",
    "        self.APP_ID = '5e13989941e72c0e039e117f'\n",
    "        self.CUSTOM_KNACK_ENDPOINT = 'knack.aft.org'\n",
    "        \n",
    "        # HTTP REQUESTS #\n",
    "        self.GET_HEADERS = {'X-Knack-REST-API-KEY':self.API_KEY,'X-Knack-Application-Id':self.APP_ID}\n",
    "        self.POST_HEADERS = {'X-Knack-REST-API-KEY':self.API_KEY,'X-Knack-Application-Id':self.APP_ID,'content-type':'application/json'}\n",
    "        self.API_URL = f'https://api.{self.CUSTOM_KNACK_ENDPOINT}/{self.API_VERSION}/'\n",
    "        self.LOADER_URL = f'https://loader.{self.CUSTOM_KNACK_ENDPOINT}/{self.API_VERSION}/applications/{self.APP_ID}'\n",
    "\n",
    "        # INTERNAL #\n",
    "        self.APP_DICT = {}\n",
    "\n",
    "        \n",
    "    # function to return key for any value\n",
    "    def get_key(self, dictionary ,val):\n",
    "        for key, value in dictionary.items():\n",
    "            if val == value:\n",
    "                return key\n",
    "\n",
    "        return ''\n",
    "    \n",
    "    def loader(self):\n",
    "        res = requests.get(url=self.LOADER_URL)\n",
    "        objects = res.json()['application']['objects']\n",
    "\n",
    "        for obj in objects:\n",
    "            fields = {}\n",
    "            name = obj['name']\n",
    "            key = obj['key']\n",
    "\n",
    "            if 'Entity-' in name:\n",
    "                for item in obj['fields']:\n",
    "                    fields.update({item['name']:item['key']})\n",
    "                self.APP_DICT.update({name.replace('Entity-', ''):{'id':key,'fields':fields}})\n",
    "    \n",
    "    # GET and format json from requestURL\n",
    "    def getJSON(self, url):\n",
    "        r = requests.get(url = self.API_URL + url, headers = self.GET_HEADERS)\n",
    "        return r.json()\n",
    "    \n",
    "    def getObjectJSON(self, object_name):\n",
    "        return (self.getJSON('objects/' + self.APP_DICT[object_name]['id']))['object']\n",
    "        \n",
    "    def find_matches(self, object_name, field_name, match_val):\n",
    "        field_id = self.APP_DICT[object_name]['fields'][field_name]\n",
    "        object_id = self.APP_DICT[object_name]['id']\n",
    "        \n",
    "        match_filter = {'match':'and', 'rules':[{'field':field_id, 'operator':'is', 'value': match_val}]}\n",
    "        filter_for_url = urllib.parse.quote(json.dumps(match_filter))\n",
    "        request_url = \"objects/\" + object_id + \"/records?filters=\" + filter_for_url\n",
    "        res = self.getJSON(request_url)\n",
    "        if res[\"total_records\"] == 0:\n",
    "            return ''\n",
    "        else:\n",
    "            return res[\"records\"]\n",
    "            \n",
    "    def convert_fields_ids2name(self, object_name, ids):\n",
    "        field_dict = self.APP_DICT[object_name]['fields']\n",
    "        out_dict = {}\n",
    "        \n",
    "        for k, v in ids.items():\n",
    "            if 'field' in k:\n",
    "                if 'raw' in k:\n",
    "                    newk = k.replace('_raw', '')\n",
    "                    key = self.get_key(field_dict, newk)\n",
    "                    if key:\n",
    "                        out_dict.update({(key+'_raw'):v})\n",
    "                    else:\n",
    "                        out_dict.update({k:v})\n",
    "                else:\n",
    "                    key = self.get_key(field_dict, k)\n",
    "                    if key:\n",
    "                        out_dict.update({key:v})\n",
    "                    else:\n",
    "                        out_dict.update({k:v})\n",
    "            else:\n",
    "                out_dict.update({k:v})\n",
    "        return out_dict\n",
    "    \n",
    "    def convert_fields_name2ids(self, object_name, names):\n",
    "        for k, v in names.items():\n",
    "            if 'field' in k:\n",
    "                key = self.APP_DICT[object_name]['fields']\n",
    "            print(k)\n",
    "            print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbfe633a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'names.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20268/402949119.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'names.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m      \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m      \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'names.csv'"
     ]
    }
   ],
   "source": [
    "# JSON PRINT HELPER #\n",
    "def jprint(output):\n",
    "    print(json.dumps(output, indent=4))\n",
    "\n",
    "\n",
    "# # GLOBAL VARIABLES #\n",
    "# YESTERDAY_DATE = (date.today() - timedelta(days = 1)).strftime(\"%m/%d/%Y\")\n",
    "# ENTITY_LIST = ['LocalDues','Employer','LocalAgreement','Unit','LocalJobClass','JobTitle','WorkLocation','WorkStructure']\n",
    "\n",
    "# client = KnackAFT()\n",
    "# client.loader()\n",
    "# output = client.find_matches('LocalDues', 'mdate', '08/18/2022')\n",
    "\n",
    "# jprint(client.APP_DICT)\n",
    "\n",
    "\n",
    "with open('ljc.csv', newline='') as csvfile:\n",
    "     reader = csv.DictReader(csvfile)\n",
    "     for row in reader:\n",
    "         print(row['first_name'], row['last_name'])\n",
    "\n",
    "# if output:\n",
    "#     for item in output:\n",
    "#         t = client.convert_fields_ids2name('LocalDues', item)\n",
    "#         jprint(t)\n",
    "#         jprint(client.convert_fields_name2ids('LocalDues', t))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# LIST OF METHODS FROM CLIENT\n",
    "\n",
    "# - CT.init()\n",
    "# - CT.loader()\n",
    "# - CT.getJson(url)\n",
    "# - CT.getObjectJSON(name)\n",
    "# - CT.find_matches(object_name, field, value)\n",
    "# - CT.convert_fields_ids2name(self, object_name, ids)\n",
    "# - CT.convert_fields_name2ids(self, object_name, names)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# output = C.find_matches('Unit', 'mdate', '08/11/2022')\n",
    "# output = C.find_matches('Unit', 'mdate', '08/11/2022')\n",
    "# output = C.find_matches('Unit', 'mdate', '08/11/2022')\n",
    "# output = C.find_matches('Unit', 'mdate', '08/11/2022')\n",
    "\n",
    "\n",
    "        \n",
    "#pl = CLIENT.getObjectPayload('Unit')\n",
    "#print(json.dumps(output, indent=4)) \n",
    "#print(json.dumps(dictout['Entity-Employer'], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8429fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_dict = {\n",
    "    'LocalDuesCategory':{\n",
    "        'extras_array':[False, True, False],\n",
    "        'sort_array':['User','Action','KnackFieldGUID','LocalDuesCategoryID','LocalDuesCategoryName','StatePerCapitaID','NationalPerCapitaID','LocalDuesAmount','LocalDuesPercentage','CreatedAt','UpdatedAt','DeletedAt','UpdateIndividualAffiliateToID','UpdateIndividualAffiliateToName','UpdateIndividualAffiliateToKnackGUID','AffiliateID','ProcessedInAFTDB']\n",
    "    },\n",
    "    'Employer':{\n",
    "        'extras_array':[False, False, True],\n",
    "        'sort_array':['User','Action','KnackFieldGUID','EmployerID','EmployerName','EmployerTypeID','ParentEmployerID','ParentEmployerName','ParentEmployerKnackGUID','Acronym','EmployerCode','Area','ChapterID','HasPrivateSector','IsStructural','IsUnknown','CreatedAt','UpdatedAt','DeletedAt','AffiliateID','ProcessedInAFTDB']\n",
    "    },\n",
    "    'LocalAgreement':{\n",
    "        'extras_array':[True, False, False],\n",
    "        'sort_array':['User','Action','KnackFieldGUID','LocalAgreementID','LocalAgreementName','EmployerID','EmployerName','EmployerKnackGUID','LocalAgreementTypeID','IsStructural','IsUnknown','CreatedAt','UpdatedAt','DeletedAt','AffiliateID','ProcessedInAFTDB']\n",
    "    },\n",
    "    'Unit':{\n",
    "        'extras_array':[True, False, False],\n",
    "        'sort_array':['User','Action','KnackFieldGUID','UnitID','UnitName','UnitTypeID','DivisionID','LocalAgreementID','LocalAgreementName','LocalAgreementKnackGUID','IsStructural','IsUnknown','CreatedAt','UpdatedAt','DeletedAt','AffiliateID','ProcessedInAFTDB']\n",
    "    },\n",
    "    'LocalJobClass':{\n",
    "        'extras_array':[True, True, False],\n",
    "        'sort_array':['User','Action','KnackFieldGUID','LocalJobClassID','LocalJobClassName','NationaJobClassID','UnitID','UnitName','UnitKnackGUID','IsStructural','IsUnknown','CreatedAt','UpdatedAt','DeletedAt','UpdateIndividualEmployerToID','UpdateIndividualEmployerToName','UpdateIndividualEmployerToKnackGUID','AffiliateID','ProcessedInAFTDB']\n",
    "    },\n",
    "    'JobTitle':{\n",
    "        'extras_array':[True, True, False],\n",
    "        'sort_array':['User','Action','KnackFieldGUID','JobTitleID','JobTitleName','LocalJobClassID','LocalJobClassName','LocalJobClassKnackGUID','IsStructural','IsUnknown','CreatedAt','UpdatedAt','DeletedAt','UpdateIndividualEmployerToID','UpdateIndividualEmployerToName','UpdateIndividualEmployerToKnackGUID','AffiliateID','ProcessedInAFTDB']\n",
    "    },\n",
    "    'WorkLocation':{\n",
    "        'extras_array':[True, True, True],\n",
    "        'sort_array':['User','Action','KnackFieldGUID','WorkLocationID','WorkLocationName','WorkLocationTypeID','NationalInstitutionTypeID','ParentWorkLocationID','ParentWorkLocationName','ParentWorkLocationKnackGUID','EmployerID','EmployerName','EmployerKnackGUID','WorkLocationCode','WorkLocationArea','CreatedAt','UpdatedAt','DeletedAt','UpdateIndividualEmployerToID','UpdateIndividualEmployerToName','UpdateIndividualEmployerToKnackGUID','AffiliateID','ProcessedInAFTDB']\n",
    "    },\n",
    "    'WorkStructure':{\n",
    "        'extras_array':[True, True, True],\n",
    "        'sort_array':['User','Action','KnackFieldGUID','WorkStructureID','WorkStructureName','WorkStructureTypeID','ParentWorkStructureID','ParentWorkStructureName','ParentWorkStructureKnackGUID','EmployerID','EmployerName','EmployerKnackGUID','WorkStructureCode','CreatedAt','UpdatedAt','DeletedAt','UpdateIndividualEmployerToID','UpdateIndividualEmployerToName','UpdateIndividualEmployerToKnackGUID','AffiliateID','ProcessedInAFTDB']\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_records_updated_at_date(knack_object, date):\n",
    "    #Convert to IDs\n",
    "    knack_object_id = find_object_id(knack_object)\n",
    "    field_to_match_id = find_field_id(knack_object, \"mdate\")\n",
    "    \n",
    "    #Get Id\n",
    "    match_filter = {'match':'and', 'rules':[{'field':field_to_match_id, 'operator':'is', 'value': date}]}\n",
    "    filter_for_url = urllib.parse.quote(json.dumps(match_filter))\n",
    "    request_url = \"https://api.knack.aft.org/v1/objects/\" + knack_object_id + \"/records?filters=\" + filter_for_url\n",
    "    \n",
    "    r = requests.get(url = request_url, headers = GET_HEADERS)\n",
    "    #print(json.dumps(r.json(), indent=4))\n",
    "    res_json_dict = json.loads(json.dumps(r.json()))\n",
    "    if res_json_dict[\"total_records\"] == 0:\n",
    "        return ''\n",
    "    else:\n",
    "        return res_json_dict[\"records\"]\n",
    "\n",
    "    \n",
    "def find_connection_payload(name, lookup):\n",
    "    output = {}\n",
    "    knack_object_id = find_object_id(name)\n",
    "    if lookup:\n",
    "        request_url = \"https://api.knack.aft.org/v1/objects/\" + knack_object_id + \"/records/\" + lookup\n",
    "        r = requests.get(url = request_url, headers = GET_HEADERS)\n",
    "        output = json.loads(json.dumps(r.json()))\n",
    "        return output\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "\n",
    "def getFields(record, object_name):\n",
    "    obj = knackmappingdict[object_name]\n",
    "    output = {}\n",
    "    \n",
    "    conn_values = obj['fields']\n",
    "    conn_values.update({(object_name+'ID'):obj['ID'], 'KnackFieldGUID':obj['GUID']})\n",
    "    \n",
    "    for k,v in conn_values.items():\n",
    "        if record[v]:\n",
    "            data = record[v]\n",
    "        else:\n",
    "            data = ''\n",
    "        output.update({k:data})\n",
    "        \n",
    "    output['UpdatedAt'] = output.pop('mdate')\n",
    "    \n",
    "    if output[(object_name+'ID')]:\n",
    "        if output['DeletedAt']:\n",
    "            output.update({'Action':'Delete'})\n",
    "        else:\n",
    "            output.update({'Action':'Update'})\n",
    "    else:\n",
    "        if output['DeletedAt']:\n",
    "            output = {}\n",
    "        else:\n",
    "            output.update({'Action':'Insert'})\n",
    "    if output:\n",
    "        output.update({'ProcessedInAFTDB':'0'})\n",
    "    #print(yaml.dump(dict_out, default_flow_style=False))\n",
    "    return output\n",
    "\n",
    "\n",
    "def getConnections(record, object_name):\n",
    "    obj = knackmappingdict[object_name]\n",
    "    output = {}\n",
    "    \n",
    "    conn_values = obj['connections']\n",
    "    \n",
    "    for k,v in conn_values.items():\n",
    "        fieldval = v.replace('_raw', '')\n",
    "        res = ''\n",
    "        knack_object = k.replace('Entity-', '')\n",
    "        if record[fieldval]:\n",
    "            data = record[v][0]['id']\n",
    "            field_id = knackmappingdict[knack_object]['ID']\n",
    "            res = find_connection_payload(knack_object, data)[field_id]\n",
    "        output.update({(knack_object+'ID'):res})\n",
    "        \n",
    "    output['User'] = output.pop('UserID')\n",
    "    return output\n",
    "    \n",
    "\n",
    "def getMappedFields(record, object_name):\n",
    "    conn_values = knackmappingdict[object_name]['mapped_fields']\n",
    "    for k,v in conn_values.items():\n",
    "        entity = k.replace('Entity-', '')\n",
    "        output = {entity+'ID':'',entity+'Name':'', entity+'KnackGUID':''}\n",
    "        fieldval = v.replace('_raw', '')\n",
    "        if record[v]:\n",
    "            data = record[v][0]['id']\n",
    "                \n",
    "            field_id = knackmappingdict[entity]['ID']\n",
    "            field_guid = knackmappingdict[entity]['GUID']\n",
    "            field_name = knackmappingdict[entity]['fields'][(entity+'Name')]\n",
    "                \n",
    "            res = find_connection_payload(entity, data)\n",
    "            out_id = res[field_id]\n",
    "            out_guid = res[field_guid]\n",
    "            out_name = res[field_name]\n",
    "                \n",
    "            output.update({entity+'ID':out_id,entity+'Name':out_name, entity+'KnackGUID':out_guid})\n",
    "    \n",
    "    #print(yaml.dump(dict_out, default_flow_style=False))\n",
    "    return output\n",
    "\n",
    "\n",
    "def getParent(record, object_name):\n",
    "    output = {'Parent'+object_name+'ID':'','Parent'+object_name+'Name':'', 'Parent'+object_name+'KnackGUID':''}\n",
    "    conn_values = knackmappingdict[object_name]['parent_fields']\n",
    "    for k,v in conn_values.items():\n",
    "        if 'Parent' in k:\n",
    "            fieldval = v.replace('_raw', '')\n",
    "            if record[fieldval]:\n",
    "                data = record[v][0]['id']\n",
    "                \n",
    "                field_id = knackmappingdict[object_name]['ID']\n",
    "                field_guid = knackmappingdict[object_name]['GUID']\n",
    "                field_name = knackmappingdict[object_name]['fields'][(object_name+'Name')]\n",
    "                \n",
    "                res = find_connection_payload(object_name, data)\n",
    "                out_id = res[field_id]\n",
    "                out_guid = res[field_guid]\n",
    "                out_name = res[field_name]\n",
    "                \n",
    "                output.update({'Parent'+object_name+'ID':out_id,'Parent'+object_name+'Name':out_name, 'Parent'+object_name+'KnackGUID':out_guid})\n",
    "    \n",
    "    #print(yaml.dump(dict_out, default_flow_style=False))\n",
    "    return output\n",
    "\n",
    "\n",
    "def getMoveTo(record, object_name):\n",
    "    if object_name == 'LocalDuesCategory':\n",
    "        phelper = 'Affiliate'\n",
    "    else:\n",
    "        phelper = 'Employer'\n",
    "        \n",
    "    output = {'UpdateIndividual'+phelper+'ToID':'','UpdateIndividual'+phelper+'ToName':'', 'UpdateIndividual'+phelper+'ToKnackGUID':''}\n",
    "    conn_values = knackmappingdict[object_name]['move_to_fields']\n",
    "    for k,v in conn_values.items():\n",
    "        if 'MoveTo' in k:\n",
    "            fieldval = v.replace('_raw', '')\n",
    "            if record[fieldval]:\n",
    "                data = record[v][0]['id']\n",
    "                \n",
    "                field_id = knackmappingdict[object_name]['ID']\n",
    "                field_guid = knackmappingdict[object_name]['GUID']\n",
    "                field_name = knackmappingdict[object_name]['fields'][(object_name+'Name')]\n",
    "                \n",
    "                res = find_connection_payload(object_name, data)\n",
    "                out_id = res[field_id]\n",
    "                out_guid = res[field_guid]\n",
    "                out_name = res[field_name]\n",
    "                \n",
    "                output.update({'UpdateIndividual'+phelper+'ToID':out_id,'UpdateIndividual'+phelper+'ToName':out_name, 'UpdateIndividual'+phelper+'ToKnackGUID':out_guid})\n",
    "    \n",
    "    #print(yaml.dump(dict_out, default_flow_style=False))\n",
    "    return output\n",
    "\n",
    "\n",
    "def payload_runner(entity_name, mapped, moveTo, parent):\n",
    "    payload = find_records_updated_at_date(entity_name, DATE_TO_RUN)\n",
    "    out = []\n",
    "    #print(yaml.dump(payload, default_flow_style=False))\n",
    "    for record in payload:\n",
    "        fields = getFields(record, entity_name)\n",
    "        connections = getConnections(record, entity_name)\n",
    "        output1 = {**fields, **connections}\n",
    "\n",
    "        if (mapped):\n",
    "            \n",
    "            # MAPPED FIELDS\n",
    "            output2 = getMappedFields(record, entity_name)\n",
    "            output1 = {**output1, **output2}\n",
    "        if (moveTo):\n",
    "            \n",
    "            # MOVE TO FIELDS\n",
    "            output2 = getMoveTo(record, entity_name)\n",
    "            output1 = {**output1, **output2}\n",
    "        if (parent):\n",
    "            \n",
    "            # PARENT FIELDS\n",
    "            output2 = getParent(record, entity_name)\n",
    "            output1 = {**output1, **output2}\n",
    "        \n",
    "        if fields:\n",
    "            out.append(output1)\n",
    "    return pd.DataFrame(out, dtype=str) \n",
    "            \n",
    "def entity_runner(entity):\n",
    "    print('Running Entity: '+entity+'...')\n",
    "\n",
    "    extras = sorting_dict[entity]['extras_array']\n",
    "    sort = sorting_dict[entity]['sort_array']\n",
    "\n",
    "    output_df = payload_runner(entity, extras[0], extras[1], extras[2])\n",
    "    rows = output_df.shape[0]\n",
    "    columns = output_df.shape[1]\n",
    "\n",
    "    if rows == 0 or columns == 0:\n",
    "        print('No Changes found for '+entity+'!')\n",
    "        print('Skipping write to civis...')\n",
    "    else:\n",
    "        print('Found '+str(rows)+' rows!')\n",
    "\n",
    "        output_df = output_df[sort]\n",
    "        entity = entity.lower()\n",
    "        table_name = os.environ['table_schema'] + entity\n",
    "\n",
    "        print('Fake writing to civis table: ' + table_name + '...')\n",
    "        print(output_df)\n",
    "        print('Done with debug output!')\n",
    "\n",
    "def main():\n",
    "    list_to_run =['LocalDuesCategory','Employer','LocalAgreement','Unit','LocalJobClass','JobTitle','WorkLocation','WorkStructure']\n",
    "    for entity in list_to_run:\n",
    "        print(\"================================\")\n",
    "        entity_runner(entity)\n",
    "    print(\"================================\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ea50e146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement yaml (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for yaml\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/3.1.14_1/libexec/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e6ddbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
